{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 1 : Data Science in Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required Readings:** \n",
    "* Chapter 1 and Chapter 9 of the book [Mining the Social Web](http://cdn.oreillystatic.com/oreilly/booksamplers/9781449367619_sampler.pdf) \n",
    "* The codes for [Chapter 1](http://bit.ly/1qCtMrr) and [Chapter 9](http://bit.ly/1u7eP33)\n",
    "* [TED Talks](https://www.ted.com/talks) for examples of 10 minutes talks.\n",
    "\n",
    "\n",
    "** NOTE **\n",
    "* Please don't forget to save the notebook frequently when working in Jupyter Notebook, otherwise the changes you made can be lost.\n",
    "\n",
    "*----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: pick a data science problem that you plan to solve using Twitter Data\n",
    "* The problem should be important and interesting, which has a potential impact in some area.\n",
    "* The problem should be solvable using twitter data and data science solutions.\n",
    "\n",
    "Please briefly describe in the following cell: what problem are you trying to solve? why this problem is important and interesting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We are looking at the trending hashtags and finding out the emotional responses people have in various regions due to \n",
    "those hashtags. We can categorize regions of the country by the emotional words used in tweets. This problem is interesting\n",
    "because you can find out how regions feel about specific issues and trends by the way people react to them on twitter. This is\n",
    "important because the data could be used by companies for advertising or politicians to find out how people feel about certain social\n",
    "issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection: Download Twitter Data using API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to solve the above problem, you need to collect some twitter data. You could select a topic that is relevant to your problem, and use Twitter API to download the relevant tweets. It is recommended that the number of tweets should be larger than 200, but smaller than 1 million.\n",
    "* Store the tweets you downloaded into a local file (txt file or json file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter, json, operator, sys\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "#---------------------------------------------\n",
    "# Define a Function to Login Twitter API\n",
    "def oauth_login():\n",
    "    # Go to http://twitter.com/apps/new to create an app and get values\n",
    "    # for these credentials that you'll need to provide in place of these\n",
    "    # empty string values that are defined as placeholders.\n",
    "    # See https://dev.twitter.com/docs/auth/oauth for more information \n",
    "    # on Twitter's OAuth implementation.\n",
    "    \n",
    "    CONSUMER_KEY = 'htBMGArjlGDW8FXLlb9iuXHW4'\n",
    "    CONSUMER_SECRET ='udmNO1AFXMi1B5ZEmlmn0jY8oudyKkinXsa3ZpjzfMUwP0Txqj'\n",
    "    OAUTH_TOKEN = '361006898-N3gjDZDTEvM6s4SQPNZGuJJN1e7HSGcnuHKr578l'\n",
    "    OAUTH_TOKEN_SECRET = '0BNDtw48dwzu3PyWRd0zh2jMP977xGIy05rIQ1GAbQNJt'\n",
    "    \n",
    "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    \n",
    "    twitter_api = twitter.Twitter(auth=auth)\n",
    "    return twitter_api\n",
    "\n",
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "twitter_api = oauth_login()\n",
    "query = '#GorsuchHearing' #Popular right now\n",
    "question = {}\n",
    "counter1 = 0\n",
    "lowestID = 0\n",
    "#<insert your code here>\n",
    "#   Please add comments to explain the general idea of each block of the code.\n",
    "for i in range (1, 3):\n",
    "    if i == 1:\n",
    "        tempQuestion = twitter_api.search.tweets(q=query, count=100)\n",
    "    else:\n",
    "        tempQuestion = twitter_api.search.tweets(q=query, count=100, max_id=lowestID)\n",
    "    #didn't quite get this to build properly, got 100 tweets\n",
    "    question = dict(question.items() + tempQuestion.items())\n",
    "    for result in question[\"statuses\"]:\n",
    "        if result['id'] < lowestID or lowestID == 0:\n",
    "            lowestID = result['id']\n",
    "\n",
    "for result in question[\"statuses\"]:\n",
    "     counter1 += 1\n",
    "     # print \"(%s) @%s %s\" % (result[\"created_at\"], result[\"user\"][\"screen_name\"], result[\"text\"])\n",
    "\n",
    "\n",
    "json_file_name = 'Results.json'\n",
    "#---------\n",
    "list_tweets = question\n",
    "print list_tweets\n",
    "\n",
    "#------------\n",
    "\n",
    "with open(json_file_name, 'w') as outfile: #output the JSON in a dump, just the statuses\n",
    "    json.dump(list_tweets[\"statuses\"], outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report  statistics about the tweets you collected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_tweets = counter1\n",
    "print 'Total acquired tweets: {}'.format(number_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration: Exploring the Tweets and Tweet Entities\n",
    "\n",
    "**(1) Word Count:** \n",
    "* Load the tweets you collected in the local file (txt or json)\n",
    "* compute the frequencies of the words being used in these tweets. \n",
    "* Plot a table of the top 30 most-frequent words with their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, operator\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "json_file_name = 'Results.json'\n",
    "\n",
    "# Load JSON file of tweets\n",
    "result = []\n",
    "new_list_of_tweets = json.loads(open(json_file_name, \"r\").read())\n",
    "\n",
    "wordDict = dict();\n",
    "for i in range (0, counter1):\n",
    "    # Get text of tweet\n",
    "    tempList = new_list_of_tweets[i][\"text\"].lower().split()\n",
    "    \n",
    "    # Add words to list\n",
    "    for word in tempList:\n",
    "        if word in wordDict:\n",
    "            wordDict[word] += 1\n",
    "        else:\n",
    "            wordDict[word] = 1\n",
    "\n",
    "table = PrettyTable(['Word', 'Count'])\n",
    "\n",
    "# Sort words\n",
    "sorted_wordDict = list(reversed(sorted(wordDict.items(), key=operator.itemgetter(1))))\n",
    "tempCounter = 0\n",
    "\n",
    "# Add rows to table\n",
    "while tempCounter < 30:\n",
    "    scrubbed_word = sorted_wordDict[tempCounter][0]  \n",
    "    table.add_row([scrubbed_word,sorted_wordDict[tempCounter][1]])\n",
    "    tempCounter += 1\n",
    "\n",
    "print table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (2) Find the most popular tweets in your collection of tweets**\n",
    "\n",
    "Please plot a table of the top 10 most-retweeted tweets in your collection, i.e., the tweets with the largest number of retweet counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get retweeted tweets\n",
    "Associations = dict()\n",
    "for i in range (0, counter1):\n",
    "    Associations[new_list_of_tweets[i][\"text\"]] = new_list_of_tweets[i][\"retweet_count\"]\n",
    "\n",
    "# Sort by count\n",
    "sorted_Associations = list(reversed(sorted(Associations.items(), key=operator.itemgetter(1))))\n",
    "tempCounter = 0\n",
    "\n",
    "# Print top 10\n",
    "while tempCounter < 10:\n",
    "    print sorted_Associations[tempCounter][0].encode('utf-8').strip() + \": \" + str(sorted_Associations[tempCounter][1])\n",
    "    tempCounter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3) Find the most popular Tweet Entities in your collection of tweets**\n",
    "\n",
    "Please plot the top 10 most-frequent hashtags and top 10 most-mentioned users in your collection of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hashtags = dict()\n",
    "mentioned_users = dict()\n",
    "\n",
    "# Iterate through tweets\n",
    "for i in range (0, number_tweets):\n",
    "    # Get hashtags entity\n",
    "    curr_hashtags = new_list_of_tweets[i][\"entities\"][\"hashtags\"]\n",
    "    \n",
    "    # Get mentions entity\n",
    "    curr_mentions = new_list_of_tweets[i][\"entities\"][\"user_mentions\"]    \n",
    "    \n",
    "    # Get hashtag counts\n",
    "    for j in range(0, len(curr_hashtags)):\n",
    "        hashtag_text = curr_hashtags[j][\"text\"]\n",
    "        if hashtag_text in hashtags:\n",
    "            hashtags[hashtag_text] += 1\n",
    "        else:\n",
    "            hashtags[hashtag_text] = 1\n",
    "            \n",
    "    # Get mentions counts\n",
    "    for j in range(0, len(curr_mentions)):\n",
    "        mention_text = curr_mentions[j][\"screen_name\"]\n",
    "        if mention_text in mentioned_users:\n",
    "            mentioned_users[mention_text] += 1\n",
    "        else:\n",
    "            mentioned_users[mention_text] = 1\n",
    "\n",
    "# Sort entities\n",
    "sorted_hashtags = list(reversed(sorted(hashtags.items(), key=operator.itemgetter(1))))\n",
    "sorted_mentioned_users = list(reversed(sorted(mentioned_users.items(), key=operator.itemgetter(1))))\n",
    "\n",
    "# Get min of entity count and 10\n",
    "hashtag_item_count = len(sorted_hashtags) if len(sorted_hashtags) < 10 else 10\n",
    "mentioned_users_item_count = len(sorted_mentioned_users) if len(sorted_mentioned_users) < 10 else 10\n",
    "\n",
    "# Hashtags graph\n",
    "hashtags_texts = [hash[0] for hash in sorted_hashtags[0:10]]\n",
    "hashtags_counts = [hash[1] for hash in sorted_hashtags[0:10]]\n",
    "\n",
    "# Get colors\n",
    "cmap = plt.get_cmap('gnuplot')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, len(hashtags_texts))]\n",
    "\n",
    "hashtags_len = range(len(hashtags_texts))\n",
    "plt.bar(hashtags_len, hashtags_counts, align='center', color=colors)\n",
    "locs, labels = plt.xticks(hashtags_len, hashtags_texts)\n",
    "plt.setp(labels, rotation=90)\n",
    "plt.title('Histogram of Hashtags')\n",
    "plt.show()\n",
    "\n",
    "# User mentions graph\n",
    "user_mention_texts = [hash[0] for hash in sorted_mentioned_users[0:10]]\n",
    "user_mention_counts = [hash[1] for hash in sorted_mentioned_users[0:10]]\n",
    "\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, len(user_mention_texts))]\n",
    "\n",
    "user_mention_len = range(len(user_mention_texts))\n",
    "plt.bar(user_mention_len, user_mention_counts, align='center', color=colors)\n",
    "locs, labels = plt.xticks(user_mention_len, user_mention_texts)\n",
    "plt.setp(labels, rotation=90)\n",
    "plt.title('Histogram of User Mentions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a histogram of the number of user mentions in the list using the following bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "bins_dict = dict()\n",
    "\n",
    "# Initialize bins\n",
    "for bin in bins:\n",
    "    bins_dict[bin] = 0\n",
    "\n",
    "# Put values in bins\n",
    "for user in sorted_mentioned_users:\n",
    "    count = user[1]\n",
    "    bin_count = (count / 10) * 10\n",
    "    \n",
    "    bins_dict[bin_count] += 1\n",
    "\n",
    "# Sort bins\n",
    "sorted_bins = list((sorted(bins_dict.items(), key=operator.itemgetter(0))))\n",
    "\n",
    "# Graph histogram\n",
    "bin_ids = [bin[0] for bin in sorted_bins]\n",
    "bin_vals = [bin[1] for bin in sorted_bins]\n",
    "\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, len(bin_ids))]\n",
    "\n",
    "bins_len = range(len(bin_ids))\n",
    "plt.bar(bins_len, bin_vals, align='center', color=colors)\n",
    "locs, labels = plt.xticks(bins_len, bin_ids)\n",
    "plt.title('Histogram of Frequencies of User Mentions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ** (4) Getting \"All\" friends and \"All\" followers of a popular user in the tweets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* choose a popular twitter user who has many followers in your collection of tweets.\n",
    "* Get the list of all friends and all followers of the twitter user.\n",
    "* Plot 20 out of the followers, plot their ID numbers and screen names in a table.\n",
    "* Plot 20 out of the friends (if the user has more than 20 friends), plot their ID numbers and screen names in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tweepy\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "CONSUMER_KEY1 = 'htBMGArjlGDW8FXLlb9iuXHW4'\n",
    "CONSUMER_SECRET1 ='udmNO1AFXMi1B5ZEmlmn0jY8oudyKkinXsa3ZpjzfMUwP0Txqj'\n",
    "OAUTH_TOKEN1 = '361006898-N3gjDZDTEvM6s4SQPNZGuJJN1e7HSGcnuHKr578l'\n",
    "OAUTH_TOKEN_SECRET1 = '0BNDtw48dwzu3PyWRd0zh2jMP977xGIy05rIQ1GAbQNJt'\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY1,CONSUMER_SECRET1)\n",
    "auth.set_access_token(OAUTH_TOKEN1,OAUTH_TOKEN_SECRET1)\n",
    "tweepyApi = tweepy.API(auth)\n",
    "ids = []\n",
    "\n",
    "# Get IDS of followers\n",
    "for name in tweepy.Cursor(tweepyApi.followers_ids, screen_name=\"funder\").items(20):\n",
    "    ids.append(name)\n",
    "\n",
    "# Get usernames from follower ids\n",
    "screen_names = [user.screen_name for user in tweepyApi.lookup_users(user_ids=ids)]\n",
    "\n",
    "# Print max 20 followers usernames and IDs\n",
    "t = PrettyTable(['Follower UserName', 'ID'])\n",
    "sizemax = 20\n",
    "\n",
    "if len(screen_names)<20:\n",
    "    sizemax = len(screen_names)\n",
    "for x in range(0,sizemax):\n",
    "    t.add_row([screen_names[x],ids[x]])\n",
    "print t\n",
    "\n",
    "# Get IDS of friends\n",
    "ids2 = []\n",
    "for name in tweepy.Cursor(tweepyApi.friends_ids, screen_name=\"funder\").items(20):\n",
    "    ids2.append(name)\n",
    "\n",
    "# Get usernames from friend ids\n",
    "screen_names2 = [user.screen_name for user in tweepyApi.lookup_users(user_ids=ids2)]\n",
    "\n",
    "# Print max 20 followers usernames and IDs\n",
    "t2 = PrettyTable(['Friend UserName', 'ID'])\n",
    "sizemax = 20\n",
    "\n",
    "if len(screen_names)<20:\n",
    "    sizemax = len(screen_names)\n",
    "for x in range(0,sizemax):\n",
    "    t2.add_row([screen_names2[x],ids2[x]])\n",
    "\n",
    "print t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Solution: implement a data science solution to the problem you are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly describe the idea of your solution to the problem in the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our solution is taking twitter data from specific hashtags and looking for specific words and hashtags in those tweets. We will take twitter location data from each of those tweets and plot regions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write codes to implement the solution in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Status,hashtags,location\n",
    "class parsedTweet:\n",
    "    def __init__(self, status, hashtags, location):\n",
    "        self.status = status\n",
    "        self.hashtags = hashtags\n",
    "        self.location = location\n",
    "\n",
    "# Not 100% perfect, but good enough, can't code for every single way people write state information\n",
    "def replaceState(text):\n",
    "    statesDIC = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "    }\n",
    "    \n",
    "    ## not exactly in alphabetical order to avoid some replacement conflicts \n",
    "    \n",
    "    states = ['Alabama','Alaska','Arizona','Arkansas','California','Colorado',\n",
    "         'Connecticut','Delaware','District of Columbia','Florida','Georgia','Hawaii','Idaho', \n",
    "         'Illinois','Indiana','Iowa','Kansas','Kentucky','Louisiana',\n",
    "         'Maine' 'Maryland','Massachusetts','Michigan','Minnesota',\n",
    "         'Mississippi', 'Missouri','Montana','Nebraska','Nevada',\n",
    "         'New Hampshire','New Jersey','New Mexico','New York',\n",
    "         'North Carolina','North Dakota','Ohio',    \n",
    "         'Oklahoma','Oregon','Pennsylvania','Rhode Island',\n",
    "         'South Carolina','South Dakota','Tennessee','Texas','Utah',\n",
    "         'Vermont','Washington','West Virginia','Virginia',\n",
    "         'Wisconsin','Wyoming']\n",
    "    \n",
    "    words = [x.strip() for x in text.split(',')]\n",
    "    replaced_string = ' '.join(str(statesDIC.get(word,word)) for word in words)\n",
    "    \n",
    "    finalString = 'NONE'\n",
    "    for statename in states:\n",
    "        if replaced_string.lower().find(statename.lower()) != -1:\n",
    "            finalString = statename\n",
    "            break\n",
    "            \n",
    "    return finalString\n",
    "\n",
    "# use this one for later on  \n",
    "parsedTweetList = []\n",
    "\n",
    "# this one is only for testing table printout\n",
    "parsedTweetListtemp= []\n",
    "\n",
    "# parse out the data we want to parse\n",
    "for result in question[\"statuses\"]:\n",
    "    hashtags = []\n",
    "    for text in result[\"entities\"].get(\"hashtags\"):\n",
    "        hashtags.append(text[\"text\"])\n",
    "    tweet = parsedTweet(result[\"text\"],hashtags,replaceState(result[\"user\"][\"location\"]))\n",
    "    tweet2 = parsedTweet(result[\"text\"],hashtags,result[\"user\"][\"location\"])\n",
    "    parsedTweetList.append(tweet)\n",
    "    parsedTweetListtemp.append(tweet)\n",
    "    parsedTweetListtemp.append(tweet2)\n",
    "\n",
    "\n",
    "## table to  show that we collected data correctly and replaced state names correctyl\n",
    "pt = PrettyTable(['Status','Hashtag', 'Location'])\n",
    "\n",
    "for parsed in parsedTweetListtemp:\n",
    "    pt.add_row([parsed.status[0:10],parsed.hashtags[0:3],parsed.location])\n",
    "\n",
    "print pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Temporary state data, to be replaced by real data\n",
    "state_data = {\n",
    "    'Alaska': [{'sentiment': 'happy', 'count': 12}, {'sentiment': 'sad', 'count': 41}, {'sentiment': 'angry', 'count': 31}],\n",
    "    'Alabama': [{'sentiment': 'happy', 'count': 22}, {'sentiment': 'sad', 'count': 2}, {'sentiment': 'angry', 'count': 12}],\n",
    "    'Arkansas': [{'sentiment': 'happy', 'count': 15}, {'sentiment': 'sad', 'count': 75}, {'sentiment': 'angry', 'count': 54}],\n",
    "    'Arizona': [{'sentiment': 'happy', 'count': 61}, {'sentiment': 'sad', 'count': 5}, {'sentiment': 'angry', 'count': 32}],\n",
    "    'California': [{'sentiment': 'happy', 'count': 19}, {'sentiment': 'sad', 'count': 66}, {'sentiment': 'angry', 'count': 12}],\n",
    "    'Colorado': [{'sentiment': 'happy', 'count': 2}, {'sentiment': 'sad', 'count': 4}, {'sentiment': 'angry', 'count': 21}]\n",
    "}\n",
    "\n",
    "# Select state for histogram\n",
    "select_state = 'Arkansas'\n",
    "\n",
    "# Sort by count\n",
    "# sorted_sentiments = list(reversed(sorted(state_data[select_state], key=operator.itemgetter('count'))))\n",
    "sorted_sentiments = state_data[select_state]\n",
    "\n",
    "# Graph sentiment histogram\n",
    "sentiments = [sent['sentiment'] for sent in sorted_sentiments]\n",
    "sentiment_counts = [sent['count'] for sent in sorted_sentiments]\n",
    "\n",
    "# Get colors\n",
    "cmap = plt.get_cmap('gnuplot')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, len(sentiments))]\n",
    "sentiment_len = range(len(sentiments))\n",
    "\n",
    "# Plot bars\n",
    "plt.bar(sentiment_len, sentiment_counts, align='center', color=colors)\n",
    "locs, labels = plt.xticks(sentiment_len, sentiments)\n",
    "plt.setp(labels, rotation=90)\n",
    "plt.title('Histogram of Sentiments in {}'.format(select_state))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For the map, we use the Basemap package, which can be found \n",
    "# here: https://sourceforge.net/projects/matplotlib/files/matplotlib-toolkits/basemap-1.0.7/\n",
    "\n",
    "# We also require the shape files for the United States, which can be found\n",
    "# here: https://github.com/matplotlib/basemap/tree/master/examples\n",
    "\n",
    "# We base the map implementation from an example, which can be found\n",
    "# here: http://stackoverflow.com/questions/7586384/color-states-with-pythons-matplotlib-basemap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.patches import Rectangle\n",
    "import sys\n",
    "\n",
    "# Get first state for each of the sentiments\n",
    "first_state = state_data[state_data.keys()[0]]\n",
    "\n",
    "# Get colors\n",
    "cmap = plt.get_cmap('gnuplot')\n",
    "colors = {}\n",
    "cmap_colors = [cmap(i) for i in np.linspace(0, 1, len(first_state))]\n",
    "\n",
    "# Make dictionary of sentiments to colors\n",
    "for i in range(len(first_state)):\n",
    "    colors[first_state[i]['sentiment']] = cmap_colors[i]\n",
    "\n",
    "def get_highest_sentinment(state_info):\n",
    "    \"\"\"\n",
    "    Returns the sentiment with the highest count.\n",
    "    \n",
    "    Arguments:\n",
    "        state_info: The array of sentiments and counts for the state.\n",
    "    Returns:\n",
    "        the sentiment\n",
    "    \"\"\"\n",
    "    max_count = -sys.maxint - 1\n",
    "    max_sentiment = None\n",
    "    \n",
    "    for i in range(len(state_info)):\n",
    "        if state_info[i]['count'] >= max_count:\n",
    "            max_count = state_info[i]['count']\n",
    "            max_sentiment = state_info[i]['sentiment']\n",
    "    \n",
    "    return max_sentiment\n",
    "\n",
    "# Build map\n",
    "map = Basemap(llcrnrlon=-119, llcrnrlat=22, urcrnrlon=-64, urcrnrlat=49, projection='lcc', lat_1=33, lat_2=45, lon_0=-95)\n",
    "\n",
    "# load shapefile\n",
    "map.readshapefile('map_data/st99_d00', name='states', drawbounds=True)\n",
    "\n",
    "# Collect names of states for lookup table\n",
    "state_names = []\n",
    "for shape_dict in map.states_info:\n",
    "    state_names.append(shape_dict['NAME'])\n",
    "\n",
    "# Axes instance\n",
    "ax = plt.gca()\n",
    "\n",
    "# Plot polygon of each state\n",
    "for state in state_data:\n",
    "    color = colors[get_highest_sentinment(state_data[state])]\n",
    "    seg = map.states[state_names.index(state)]\n",
    "    poly = Polygon(seg, facecolor=color,edgecolor='black')\n",
    "    ax.add_patch(poly)\n",
    "\n",
    "# Create legend of sentiments\n",
    "legend_rects = []\n",
    "legend_texts = []\n",
    "\n",
    "# Add color and name to legend\n",
    "for i in range(len(colors.keys())):\n",
    "    sentiment = colors.keys()[i]\n",
    "    legend_rects.append(Rectangle((0, 0), 1, 1, fc=colors[sentiment], fill=True, edgecolor='none', linewidth=0))\n",
    "    legend_texts.append(sentiment)\n",
    "\n",
    "ax.legend(legend_rects, legend_texts, loc=2, bbox_to_anchor=(1.0, 0.7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results: summarize and visualize the results discovered from the analysis\n",
    "\n",
    "Please use figures, tables, or videos to communicate the results with the audience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this Jupyter notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"jupyter notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . Each team present their case studies in class for 10 minutes.\n",
    "\n",
    "Please compress all the files in a zipped file.\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "\n",
    "        Please submit through Canvas, in the Assignment \"Case Study 1\".\n",
    "        \n",
    "** Note: Each team only needs to submit one submission in Canvas **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Peer-Review Grading Template:\n",
    "\n",
    "** Total Points: (100 points) ** Please don't worry about the absolute scores, we will rescale the final grading according to the performance of all teams in the class.\n",
    "\n",
    "Please add an \"**X**\" mark in front of your rating: \n",
    "\n",
    "For example:\n",
    "\n",
    "*2: bad*\n",
    "          \n",
    "**X** *3: good*\n",
    "    \n",
    "*4: perfect*\n",
    "\n",
    "\n",
    "    ---------------------------------\n",
    "    The Problem: \n",
    "    ---------------------------------\n",
    "    \n",
    "    1. (5 points) how well did the team describe the problem they are trying to solve using twitter data? \n",
    "       0: not clear\n",
    "       1: I can barely understand the problem\n",
    "       2: okay, can be improved\n",
    "       3: good, but can be improved\n",
    "       4: very good\n",
    "       5: crystal clear\n",
    "    \n",
    "    2. (10 points) do you think the problem is important or has a potential impact?\n",
    "        0: not important at all\n",
    "        2: not sure if it is important\n",
    "        4: seems important, but not clear\n",
    "        6: interesting problem\n",
    "        8: an important problem, which I want to know the answer myself\n",
    "       10: very important, I would be happy invest money on a project like this.\n",
    "    \n",
    "    ----------------------------------\n",
    "    Data Collection:\n",
    "    ----------------------------------\n",
    "    \n",
    "    3. (10 points) Do you think the data collected are relevant and sufficient for solving the above problem? \n",
    "       0: not clear\n",
    "       2: I can barely understand what data they are trying to collect\n",
    "       4: I can barely understand why the data is relevant to the problem\n",
    "       6: the data are relevant to the problem, but better data can be collected\n",
    "       8: the data collected are relevant and at a proper scale (> 300 tweets)\n",
    "      10: the data are properly collected and they are sufficient\n",
    "\n",
    "    -----------------------------------\n",
    "    Data Exploration:\n",
    "    -----------------------------------\n",
    "    4. How well did the team solve the following task:\n",
    "    (1) Word Count (5 points):\n",
    "       0: missing answer\n",
    "       1: okay, but with major problems\n",
    "       3: good, but with minor problems\n",
    "       5: perfect\n",
    "    \n",
    "    (2) Find the most popular tweets in your collection of tweets: (5 points)\n",
    "       0: missing answer\n",
    "       1: okay, but with major problems\n",
    "       3: good, but with minor problems\n",
    "       5: perfect\n",
    "    \n",
    "    (3) Find popular twitter entities  (5 points)\n",
    "       0: missing answer\n",
    "       1: okay, but with major problems\n",
    "       3: good, but with minor problems\n",
    "       5: perfect\n",
    "\n",
    "    (4) Find user's followers and friends (5 points)\n",
    "       0: missing answer\n",
    "       1: okay, but with major problems\n",
    "       3: good, but with minor problems\n",
    "       5: perfect\n",
    "\n",
    "    -----------------------------------\n",
    "    The Solution\n",
    "    -----------------------------------\n",
    "    5.  how well did the team describe the solution they used to solve the problem? \n",
    "       0: not clear\n",
    "       2: I can barely understand\n",
    "       4: okay, can be improved\n",
    "       6: good, but can be improved\n",
    "       8: very good\n",
    "       10: crystal clear\n",
    "       \n",
    "    6. how well is the solution in solving the problem? \n",
    "       0: not relevant\n",
    "       1: barely relevant to the problem\n",
    "       2: okay solution, but there is an easier solution.\n",
    "       3: good, but can be improved\n",
    "       4: very good, but solution is simple/old\n",
    "       5: innovative and technically sound\n",
    "       \n",
    "    7. how well did the team implement the solution in python? \n",
    "       0: the code is not relevant to the solution proposed\n",
    "       2: the code is barely understandable, but not relevant\n",
    "       4: okay, the code is clear but incorrect\n",
    "       6: good, the code is correct, but with major errors\n",
    "       8: very good, the code is correct, but with minor errors\n",
    "      10: perfect \n",
    "   \n",
    "    -----------------------------------\n",
    "    The Results\n",
    "    -----------------------------------\n",
    "     8.  How well did the team present the results they found in the data? \n",
    "       0: not clear\n",
    "       2: I can barely understand\n",
    "       4: okay, can be improved\n",
    "       6: good, but can be improved\n",
    "       8: very good\n",
    "      10: crystal clear\n",
    "       \n",
    "     9.  How do you think the results they found in the data? \n",
    "       0: not clear\n",
    "       1: likely to be wrong\n",
    "       2: okay, maybe wrong\n",
    "       3: good, but can be improved\n",
    "       4: make sense, but not interesting\n",
    "       5: make sense and very interesting\n",
    "     \n",
    "    -----------------------------------\n",
    "    The Presentation\n",
    "    -----------------------------------\n",
    "    10. How all the different parts (data, problem, solution, result) fit together as a coherent story?  \n",
    "       0: they are irrelevant\n",
    "       1: I can barely understand how they are related to each other\n",
    "       2: okay, the problem is good, but the solution doesn't match well, or the problem is not solvable.\n",
    "       3: good, but the results don't make much sense in the context\n",
    "       4: very good fit, but not exciting (the storyline can be improved/polished)\n",
    "       5: a perfect story\n",
    "      \n",
    "    11. Did the presenter make good use of the 10 minutes for presentation?  \n",
    "       0: the team didn't present\n",
    "       1: bad, barely finished a small part of the talk\n",
    "       2: okay, barely finished most parts of the talk.\n",
    "       3: good, finished all parts of the talk, but some part is rushed\n",
    "       4: very good, but the allocation of time on different parts can be improved.\n",
    "       5: perfect timing and good use of time      \n",
    "\n",
    "    12. How well do you think of the presentation (overall quality)?  \n",
    "       0: the team didn't present\n",
    "       1: bad\n",
    "       2: okay\n",
    "       3: good\n",
    "       4: very good\n",
    "       5: perfect\n",
    "\n",
    "\n",
    "    -----------------------------------\n",
    "    Overall: \n",
    "    -----------------------------------\n",
    "    13. How many points out of the 100 do you give to this project in total?  Please don't worry about the absolute scores, we will rescale the final grading according to the performance of all teams in the class.\n",
    "    Total score:\n",
    "    \n",
    "    14. What are the strengths of this project? Briefly, list up to 3 strengths.\n",
    "       1: \n",
    "       2:\n",
    "       3:\n",
    "    \n",
    "    15. What are the weaknesses of this project? Briefly, list up to 3 weaknesses.\n",
    "       1:\n",
    "       2:\n",
    "       3:\n",
    "    \n",
    "    16. Detailed comments and suggestions. What suggestions do you have for this project to improve its quality further.\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ---------------------------------\n",
    "    Your Vote: \n",
    "    ---------------------------------\n",
    "    1. [Overall Quality] Between the two submissions that you are reviewing, which team would you vote for a better score?  \n",
    "       -1: I vote the other team is better than this team\n",
    "        0: the same\n",
    "        1: I vote this team is better than the other team \n",
    "        \n",
    "    2. [Presentation] Among all the teams in the presentation, which team do you think deserves the best presentation award for this case study?  \n",
    "        1: Team 1\n",
    "        2: Team 2\n",
    "        3: Team 3\n",
    "        4: Team 4\n",
    "        5: Team 5\n",
    "        6: Team 6\n",
    "        7: Team 7\n",
    "        8: Team 8\n",
    "        9: Team 9\n",
    "       10: Team 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
